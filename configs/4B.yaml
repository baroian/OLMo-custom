# config.yaml

wandb_project: "all_in_olmo"
wandb_name: "4B_LWS"
train_data_file: "4B.npy"

#------------------------------------------------------------------------------------------------
# General Important Parameters (Top-level)
#------------------------------------------------------------------------------------------------
sequence_length: 1024
# batch_size is a fallback if 'global_batch_size' in the Training Hyperparameters section is not specified.
# It represents the number of sequences in a global batch (for one optimizer step).
batch_size: 168
micro_batch_size: 56 # Number of sequences processed per device in a single forward/backward pass.
steps: 23_000

#set to - (minus) to disable evaluation and inference
evaluation_times: 20 # run evaluation 2 times (eval_interval = steps/evaluation_interval) + one time at the beginning
inference_times: 20   # Run inference 2 times (inference_interval = steps/inference_times)

# Group Query Attention (GQA)
n_kv_heads: 4                             # Number of key/value heads for GQA. Must be <= n_heads.
                                          # Make sure build_model passes this to TransformerConfig.
# Layer-Wise Scaling parameters

learning_rate: 0.001
# INCREASE LEARNING RATE for small runs <5B tokens

fnn_scalars: [2.0, 5.3]
qkv_scalars: [0.5,2.0]
output_scalars: [1.0, 1.0]
#fnn_scalars: [1.0, 2.0]
#qkv_scalars: [0.5, 1.0]
#output_scalars: [0.8, 1.2]
verbose_scaling: True



# -----------------------------------------------------------------------------
# directories config
# -----------------------------------------------------------------------------
data_dir: ./data_local_test                # Directory for prepared data and checkpoints
save_dir: checkpoints



#------------------------------------------------------------------------------------------------
# DATA PREPARATION ARGUMENTS
#------------------------------------------------------------------------------------------------
data_preparation:
  output_file_name: "4B.npy" # Name of the tokenized file within data.data_dir
  validation_output_file_name: "c4_validation.npy"
  validation: True
  #tokenizer_name: "allenai/gpt-neox-olmo-dolma-v1_5" # assuming we're not changin tokenizer
  #hf_dataset_name: "wikipedia"
  #hf_dataset_subset: "20220301.en"
  total_tokens_to_collect: 4_000_000_000
  tokenizer_processing_batch_size: 5000  # how many articles/entries to tokenize at once
  dataset_proportions:  
    ### SCIENCE small DATASETS - always included.
    allenai/ai2_arc/ARC-Challenge: 0.1
    allenai/ai2_arc/ARC-Easy: 0.1
    allenai/sciq: 0.1

    allenai/dolmino-mix-1124/wiki:   0.15
    #allenai/dolmino-mix-1124/dclm
    allenai/dolmino-mix-1124/pes2o: 0.15
    allenai/dolmino-mix-1124/flan: 0.7

    
    ### CODE AND MATH DATASETS
    #tokyotech-llm/swallow-code/exp1-the-stack-v2: 1.0
    #math-ai/AutoMathText/code-full: 0.25
    #math-ai/AutoMathText/web-0.50-to-1.00: 0.5
    #math-ai/AutoMathText/arxiv-full: 0.5


#ds = load_dataset("allenai/ai2_arc", "ARC-Challenge")
#ds = load_dataset("allenai/ai2_arc", "ARC-Easy")
#7k rows * 100 tokens = 700k tokens

#ds = load_dataset("allenai/sciq")
#11k rows * 100 tokens = 1.1M tokens



#ds = load_dataset("allenai/ai2_arc", "ARC-Easy")  # or ARC-Challenge

# {
#     "answerKey": "B",
#     "choices": {
#         "label": ["A", "B", "C", "D"],
#         "text": ["Shady areas increased.", "Food sources increased.", "Oxygen levels increased.", "Available water increased."]
#     },
#     "id": "Mercury_SC_405487",
#     "question": "One year, the oak trees in a park began producing more acorns than usual. The next year, the population of chipmunks in the park also increased. Which best explains why there were more chipmunks the next year?"
# }

#ds = load_dataset("allenai/sciq")

#{
#    "correct_answer": "coriolis effect",
#    "distractor1": "muon effect",
#    "distractor2": "centrifugal effect",
#    "distractor3": "tropical effect",
#    "question": "What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?",
#    "support": "\"Without Coriolis Effect the global winds would blow north to south or south to north. But Coriolis makes them blow northeast to..."
#}



#------------------------------------------------------------------------------------------------
# HF CACHE PATHS - default now, change if needed
#------------------------------------------------------------------------------------------------
hf_cache_paths:
  transformers: cache/transformers
  datasets: cache/datasets
  huggingface: cache/huggingface


#------------------------------------------------------------------------------------------------
# WANDB API KEY - put your own key.
#------------------------------------------------------------------------------------------------

WANDB_API_KEY: "8e07447fa3d6269331f7ecd0b27f8518c2a65855"


# -----------------------------------------------------------------------------
# Training Hyperparameters
# -----------------------------------------------------------------------------
# The following block was previously enclosed in """ which is not valid YAML for comments.
# Each line should be prefixed with # if it's a comment.
# If these are intended to be active parameters, they should be uncommented and correctly formatted.

# steps: 20                                 # Total number of global training steps (optimizer steps)
# global_batch_size: 2048                      # Total batch size across all GPUs.
#                                           # For FSDP, this will be divided among ranks.
#                                           # Example: 2 GPUs, global_batch_size 8 -> 4 sequences per GPU per global step.
#                                           # Your previous value was 2, which is very small for a global batch.
#                                           # Adjust based on your GPU memory and number of GPUs.
# sequence_length: 1024                     # Sequence length for the model
# 
# gradient_accumulation_steps: 1            # Number of micro-steps to accumulate gradients before an optimizer step.
#                                           # Effective batch size = global_batch_size * gradient_accumulation_steps (conceptually, though OLMo applies global_batch_size per optimizer step)
#                                           # OLMo's TrainerConfig uses global_batch_size directly for the optimizer step.
#                                           # The device_train_microbatch_size is what's processed per forward/backward pass on a device.
#                                           # Set device_train_microbatch_size in TrainerConfig or let OLMo infer.
#                                           # If OLMo infers, it's often global_batch_size / world_size / gradient_accumulation_steps.

# Assuming the parameters below this line are the ones you actually want active for this section:

#learning_rate: 0.001
# INCREASE LEARNING RATE for small runs <5B tokens
weight_decay: 0.1
betas: [0.9, 0.95]                        # AdamW betas
seed: 42                                  # Base seed for reproducibility (will be adjusted per rank in train.py)



# Note: The original config had 'steps', 'global_batch_size', 'sequence_length', 
# and 'gradient_accumulation_steps' defined both at the top-level (lines 6-9) 
# and within the commented-out block above. 
# Ensure the active definitions (e.g., lines 6-9 and 66-77) reflect your intended settings.
# If the block above (lines 57-71) was meant to be active, you need to remove the # from each line.
# For now, I am assuming the top-level definitions and the learning_rate, weight_decay etc. below are the active ones.


# -----------------------------------------------------------------------------
# Model Configuration (OLMo-190M base with GQA and LWS)
# -----------------------------------------------------------------------------
# These parameters define the model architecture.
# Ensure your `utils/model.py` correctly uses these values, especially n_kv_heads,
# and custom LWS scalars if they modify the TransformerConfig or model layers.
model_dtype: "bf16"                       # Precision for model parameters and computation (e.g., "bf16", "fp32").
                                          # This will guide dtype in build_model and FSDP settings.

# OLMo-190M defaults (can be overridden if build_model passes them to TransformerConfig)
global_dim: 768                           # d_model: Dimensionality of the model
n_heads: 12                               # Number of attention heads
head_dim: 64                              # Dimensionality of each attention head (global_dim / n_heads)


# -----------------------------------------------------------------------------
# FSDP and Activation Checkpointing
# -----------------------------------------------------------------------------
# FSDP settings are largely handled by TransformerDataParallelConfig in utils/model.py
# Activation checkpointing mode can be configured if needed (e.g., "full", "fine_grained")
# Defaulting to "full" as set in utils/model.py's TransformerTrainModuleConfig.
# activation_checkpointing_mode: "full" # Example if you want to control it from here.

# -----------------------------------------------------------------------------
# Inference Callback
# -----------------------------------------------------------------------------
#inference_interval: 20                     # Run inference every N global steps
inference_prompt: "Once upon a time in a distant galaxy, there lived a curious scientist who discovered that "


# global_dim: 768
# head_dim: 64

# -----------------------------------------------------------------------------
inference_mode: "all"  # Options: "cycle", "all", "random"
                        # "cycle": Use a different prompt each time
                        # "all": Run inference on all prompts each time
                        # "random": Pick a random prompt each time

inference_prompts:
  - "In a world where quantum computing has become mainstream, a team of researchers made a groundbreaking discovery that could revolutionize how we understand consciousness. They found that "
  - "The most significant challenge facing humanity in the next century will be managing the delicate balance between technological advancement and environmental preservation. This is because "
  - "As artificial intelligence continues to evolve at an unprecedented pace, we must carefully consider its impact on human society. The key question is "
  - "The future of space exploration holds immense potential for scientific discovery and human expansion. The most promising development is "
  - "Education systems around the world need fundamental transformation to prepare students for an increasingly complex future. The most important change should be "
  - "The relationship between technology and human well-being has become increasingly complex. The most critical aspect we need to address is "
  - "Climate change presents both challenges and opportunities for human innovation. The most promising solution involves "
  - "The intersection of neuroscience and artificial intelligence is leading to remarkable discoveries about human cognition. The most fascinating finding is "
  - "Global economic systems are undergoing profound transformation in the digital age. The most significant change will be "
  - "The future of healthcare lies in the integration of traditional medicine with cutting-edge technology. The most promising development is "
